{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==3.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n",
      "C:\\Users\\Chan Ho Park\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
      "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
      "       'popularity', 'poster_path', 'production_companies',\n",
      "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
      "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
      "       'vote_average', 'vote_count'],\n",
      "      dtype='object')\n",
      "                                              genres  \\\n",
      "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
      "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
      "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
      "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
      "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Led by Woody, Andy's toys live happily in his ...   \n",
      "1  When siblings Judy and Peter discover an encha...   \n",
      "2  A family wedding reignites the ancient feud be...   \n",
      "3  Cheated on, mistreated and stepped on, the wom...   \n",
      "4  Just when George Banks has recovered from his ...   \n",
      "\n",
      "                                             tagline  \n",
      "0                                                NaN  \n",
      "1          Roll the dice and unleash the excitement!  \n",
      "2  Still Yelling. Still Fighting. Still Ready for...  \n",
      "3  Friends are the people who let you be yourself...  \n",
      "4  Just When His World Is Back To Normal... He's ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chan Ho Park\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "file_dir = \"./data/movies_metadata.csv\"\n",
    "data = pd.read_csv(file_dir)\n",
    "print(data.columns)\n",
    "data = data[['genres','overview','tagline']]\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45466, 3)\n",
      "Index(['genres', 'overview', 'tagline'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genre is 32 \n",
      "\n",
      "['Animation', 'Comedy', 'Family', 'Adventure', 'Fantasy', 'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror', 'History', 'Science Fiction', 'Mystery', 'War', 'Foreign', 'Music', 'Documentary', 'Western', 'TV Movie', 'Carousel Productions', 'Vision View Entertainment', 'Telescene Film Group Productions', 'Aniplex', 'GoHands', 'BROSTA TV', 'Mardock Scramble Production Committee', 'Sentai Filmworks', 'Odyssey Media', 'Pulser Productions', 'Rogue State', 'The Cartel']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>[Comedy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...   \n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "\n",
       "                                             tagline  \\\n",
       "0                                                NaN   \n",
       "1          Roll the dice and unleash the excitement!   \n",
       "2  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Friends are the people who let you be yourself...   \n",
       "4  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                            all  \n",
       "0   [Animation, Comedy, Family]  \n",
       "1  [Adventure, Fantasy, Family]  \n",
       "2             [Romance, Comedy]  \n",
       "3      [Comedy, Drama, Romance]  \n",
       "4                      [Comedy]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tolist(data):\n",
    "    dic = ast.literal_eval(data)  \n",
    "    out = []\n",
    "    for g in dic:\n",
    "        out.append(g['name'])\n",
    "    return out\n",
    "allg = []\n",
    "for index,row in data.iterrows():\n",
    "    dic = ast.literal_eval(row['genres'])  \n",
    "    for g in dic:\n",
    "        try:\n",
    "            allg.index(g['name'])\n",
    "        except:\n",
    "            allg.append(g['name'])\n",
    "data['all'] = data['genres'].apply(tolist)\n",
    "print(\"Number of genre is %d \\n\" %(len(allg)))\n",
    "print(allg)\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about genre \n",
    "\n",
    "### Total list of genre and number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifiying minor labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              genres  \\\n",
      "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
      "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
      "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
      "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
      "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
      "\n",
      "                                             v_genre  \n",
      "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "[1124, 8820, 524, 1514, 704, 1191, 11966, 4489, 1685, 1665, 2619, 279, 647, 554, 379, 118, 487, 3415, 451, 390, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def label2vector(genre,allg):\n",
    "    out = [0]*len(allg)\n",
    "    dic = ast.literal_eval(genre)  \n",
    "    for g in dic:\n",
    "        out[allg.index(g['name'])] = 1\n",
    "        break\n",
    "    return out\n",
    "\n",
    "data['v_genre'] = data['genres'].apply(label2vector, args=(allg,))\n",
    "print(data[['genres','v_genre']].head(5))\n",
    "\n",
    "count_g = [0]* len(allg)\n",
    "for i,row in data['v_genre'].items():\n",
    "    for j,r in zip(range(len(row)),row):\n",
    "        if r == 1:\n",
    "            count_g[j] +=1\n",
    "print(count_g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of minor label index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19730, 29503, 35587]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "rm_list = [ 'Carousel Productions', 'Vision View Entertainment', 'Telescene Film Group Productions','Aniplex', 'GoHands', 'BROSTA TV', 'Mardock Scramble Production Committee', 'Sentai Filmworks', 'Odyssey Media', 'Pulser Productions', 'Rogue State', 'The Cartel']\n",
    "rm_index = []\n",
    "for i,row in data['v_genre'].items():\n",
    "    for j,r in zip(range(len(row)),row):\n",
    "        if r == 1 and j > 19 and j <32:\n",
    "            rm_index.append(i)\n",
    "print(rm_index)\n",
    "data = data.drop(rm_index)\n",
    "data['v_genre'] = data['v_genre'].apply(lambda x: x[:20])\n",
    "print(data['v_genre'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning description's stopword\n",
    "\n",
    "- processing string data through nltk tool and stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\n",
      "\n",
      "___________________________________________________________________________\n",
      "\n",
      "After:\n",
      " lead woody andy toy live happily room andy birthday bring buzz lightyear onto scene afraid lose place andy heart woody plot buzz but circumstance separate buzz woody owner duo eventually learn put aside difference\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tag(data,lemma,stop_words):\n",
    "    filtered = \"\"\n",
    "    try:\n",
    "        data = word_tokenize(data)\n",
    "        data = nltk.pos_tag(data)\n",
    "        for word in data:\n",
    "            if word[0] not in stop_words and len(word[0])>2:\n",
    "                try: \n",
    "                    word[1].index('V')\n",
    "                    filtered = filtered + \" \" + lemma.lemmatize(word[0].lower(),'v')\n",
    "                except:\n",
    "                    filtered = filtered + \" \" + lemma.lemmatize(word[0].lower())\n",
    "    except:\n",
    "        pass  \n",
    "    return filtered\n",
    "def add(row):\n",
    "    if len(str(row['tagline'])) > 3:\n",
    "        return str(row['overview'])+\" \"+str(row['tagline'])\n",
    "    else:\n",
    "        return str(row['overview'])\n",
    "#concatenate overview and tagline\n",
    "data['description'] = data[['overview','tagline']].apply(add, axis =1)\n",
    "print(\"Before:\")\n",
    "print(data.iloc[0]['description'])\n",
    "print(\"\\n___________________________________________________________________________\\n\")\n",
    "#remove stopword\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "data['description'] = data['description'].apply(tag,args=(lemma,stop_words,))\n",
    "print(\"After:\")\n",
    "print(data.iloc[0]['description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert description to vector\n",
    "     - Using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def des2vec(data,model):\n",
    "    return model.infer_vector(word_tokenize(data))\n",
    "\n",
    "model= Doc2Vec.load(\"./models/doc2vec.bin\")\n",
    "data['v_description'] = data['description'].apply(des2vec, args=(model,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " remu kasvaa vaatimattomissa oloissa teinipoikana hän jättänyt koulunsa kesken ammatiltaan rikollinen suvun kohtalo toteutumassa odotettavissa tavallinen tarina vankila alkoholismi väkivaltainen kuolema\n",
      " ...\n",
      "\n",
      "\n",
      " москва город без прошлого без будущего город который прощает ошибок московская богема криминальный бизнес ночная жизнь легкие деньги возбуждение растерянность образуют поверхность этой жизни время очередной перевозки партии нала для нового русского кличке майкл деньги исчезают ситуация осложняется еще больше когда накануне свадьбы невеста майкла влюбляется его друга которого тот подозревает похищении денег герои пытаются заглушить страх перед разрушительной жестокостью жизни алкоголем сексом интрига продолжает разрастаться пока проблема разрешается наиболее универсальным способом пулей затылок…\n",
      " ульяна тулина необычный человек взрослом теле душа странного наивного ребенка поступки импульсивные нелепые смешат удивляют раздражают окружающих единственный родной человек который заботится ульяне сестра-двойняшка лиза актриса-неудачница хрупкая гармония отношений рушится когда доме тулиных появляется мужчина саша начинающий писатель мечтающий прославиться неожиданно для себя становится нужным одновременно обеим сестрам только одна них становится героиней соавтором музой его нового романа однако реальная жизнь литературный жанр она сама допишет необходимый финал ...\n",
      " фильм создан жанре музыкальной комедии мотивам знаменитого юмористического романа генри короли капуста действие фильма разворачивается условной стране анчурия где каждый год выбирают нового президента так все шло своим чередом островной анчурии если чемоданчик сотнями тысяч долларов вдруг обнаружившийся местного брадобрея таинственный незнакомец однажды выброшенный волной анчурский берег\n",
      " вызову своего жениха светлана поехала захолустный городок где самой окраине стоял двухэтажный дом семи ветрах началась война игорь встретил светлану она решила остаться ждать его конца войны вскоре окраине городка разместилась редакция фронтовой газеты когда немцы подошли городу дом превратился госпиталь светлана стала бойцом\n",
      "\n",
      " tupla-uuno tupla-uuno\n",
      "\n",
      " артем колчин был одним многих хотел славы выбрал свой путь стал боксером теперь артем претендент чемпионский титул боец известный всему миру большая белая надежда главный бой его жизни пошел так как ожидал теперь ему предстоит решать свои проблемы вне ринга него всего три дня чтобы расквитаться теми кто его предал защитить любимую женщину наемных убийц собрать огромную сумму денег реальном мире бьют без перчаток без правил вместо ринга злые улицы большого города нет рефери который может остановить бой боец остается бойцом знает что такое страх способен бояться продолжает биться даже когда надежда умерла счет идет часы артем начинает бой любовь друзей собственную жизнь\n",
      " рафшан джумжуд гастарбайтеры нубарашена нелегально приезжают москву где бригадир леонид получил одного олигарха заказ супердорогой ремонт потеряв столице нацайника рафшан джумжуд пытаются найти спасти его сея повсюду разрушения хаос самой безнадёжной ситуации судьба поворачивается гастарбайтерам лицом рафшан джумжуд узнают страшную тайну которая изменит всё…\n",
      " андрей простой русский оперуполномоченный борется преступностью верит сказки они ему порядком опостылели каждый подозреваемый норовит выдумать небылицу расследуя очередное незаурядное дело его коллеги попадают жуткий переплет бандиты смертельная перестрелка ... чертовщина дальнейшие события приводят андрея старый ветхий дом этом домике находящемся самом центре большого города живёт русский интеллигент чеховского типа ильин живёт себе тихо мирно поры времени знает что старом шкафу находится вход таинственное подземелье магическую комнату которой можно попасть любую столицу мира чердаке живёт добрый домовой пиня который знает все тайны дома ...\n",
      " москва наши дни центре города машины двое воришек крадут сумку миллионом евро думая что наконец повезло даже представляя что самом деле они ввязались деньгами начинается настоящая охота перед искушением может устоять никто сумма виде двух пачек денег полмиллиона евро делает абсолютно разных людей одинаковыми своих поступках желаниях началось все того что миллионщик григорий захотел честно платить налоги\n",
      " главный герой александр крупный бизнесмен влиятельный человек своем городе после напряженного рабочего дня везет своего друга помощника дмитрия ресторан удивлению дмитрия там ждут деловые партнеры друзья помещении вообще нет посетителей мужчинам придется провести время компании молчаливых официантов многочисленных бутылок содержимым различной крепости когда дверь запирается изнутри двух героев выяснение отношений остается целая ночь\n",
      " девушки даши приехавшей подругой покорять москву редкая специальность преподаватель техники речи жизнь самая обыкновенная съемная квартира невысокие гонорары занятия утра вечера однажды даша получает выгодное предложение дать уроки преуспевающему бизнесмену владу участвующему политических выборах героев начинается бурный роман случайная встреча женой влада заставляет дашу взглянуть происходящее совсем другой стороны влад оказывается совсем героем романа вовсе мужчиной мечты…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.05782209,  0.03871881,  0.00344013,  0.08453442,  0.00065249,\n",
       "        0.01060922,  0.05268787, -0.04557037,  0.05272275,  0.08696493,\n",
       "       -0.02750796, -0.11166745, -0.02641165, -0.06777664, -0.09281522,\n",
       "        0.05298287,  0.07704163,  0.10824221,  0.03521583, -0.05674635,\n",
       "        0.05337234, -0.00189427,  0.10666484,  0.02248201,  0.00968474,\n",
       "       -0.0568651 , -0.07252975,  0.08453151,  0.09002177, -0.05923752,\n",
       "       -0.05219232, -0.01393781, -0.04170845,  0.01729693, -0.06512742,\n",
       "       -0.00980196,  0.00528826,  0.00168573,  0.04685647,  0.09290949,\n",
       "        0.10045006, -0.04267447,  0.12362526,  0.03561183,  0.00129918,\n",
       "       -0.07345654, -0.00184195,  0.0200693 ,  0.02873839,  0.04418001,\n",
       "       -0.06099229,  0.11389596,  0.0183229 , -0.05935669,  0.01588694,\n",
       "       -0.00556001,  0.00986008, -0.18343608,  0.04042853, -0.04620434,\n",
       "        0.00703076,  0.04824611, -0.01732527, -0.0471671 , -0.07436553,\n",
       "       -0.08427066,  0.00455439,  0.01441919, -0.03253428,  0.13968186,\n",
       "        0.09002177,  0.01585524,  0.06788126, -0.0071244 , -0.07991318,\n",
       "       -0.04710751,  0.03262039,  0.03671101,  0.00841843,  0.03296879,\n",
       "        0.00231207, -0.02927253,  0.00521478, -0.02015795,  0.01407914,\n",
       "        0.01751709, -0.10173979,  0.06852504,  0.00566101,  0.04951477,\n",
       "        0.01437887,  0.02422333, -0.09743936, -0.06567192, -0.01742517,\n",
       "       -0.01640829,  0.03438168,  0.02698117,  0.03820001, -0.03092739,\n",
       "       -0.04624285, -0.04451352,  0.06096613, -0.02759198, -0.0298006 ,\n",
       "       -0.06619263,  0.00838797, -0.02178337,  0.01969764, -0.08624776,\n",
       "       -0.01948039,  0.01007153, -0.00876472, -0.00960432,  0.09208552,\n",
       "       -0.03775914,  0.00917126, -0.0218593 ,  0.05016713, -0.01884378,\n",
       "       -0.04366085, -0.04070754, -0.06656701,  0.03381638, -0.03909674,\n",
       "       -0.01958429,  0.00054205,  0.02287075,  0.0172348 ,  0.00049073,\n",
       "       -0.02625384, -0.04492587, -0.06148747,  0.02665202, -0.00129918,\n",
       "        0.00677187,  0.01669203,  0.00837853, -0.02895555,  0.07966977,\n",
       "        0.05281676, -0.11279006, -0.05460685,  0.05189006,  0.0536074 ,\n",
       "        0.08850824, -0.02096376, -0.017318  , -0.07119352, -0.08041018,\n",
       "        0.08453369, -0.02557809, -0.09414055,  0.11020188, -0.01947603,\n",
       "       -0.0679047 , -0.04462387, -0.11795117, -0.08752333, -0.04155986,\n",
       "       -0.03166635,  0.04334041,  0.07853445, -0.09181722, -0.00180163,\n",
       "       -0.12309001,  0.10754613, -0.03405426, -0.02833993,  0.02522214,\n",
       "       -0.07341222, -0.0279813 , -0.02860641, -0.05191476, -0.00281098,\n",
       "       -0.03130988,  0.07749993, -0.05252348, -0.01914687, -0.01799665,\n",
       "       -0.07824707, -0.06733485,  0.05551774, -0.05056572, -0.0271839 ,\n",
       "       -0.02907017,  0.00118251,  0.01283773,  0.08269464,  0.01340685,\n",
       "        0.08557274, -0.00482523,  0.02519771, -0.01879265, -0.03229832,\n",
       "        0.07273065, -0.03328741, -0.04183923, -0.13810222, -0.06496321,\n",
       "        0.05243465,  0.01201775, -0.0135211 , -0.0704927 ,  0.03764507,\n",
       "       -0.06941114,  0.01418704,  0.00201725, -0.0617683 , -0.07694862,\n",
       "       -0.06490362,  0.08935256, -0.05417306, -0.02944874, -0.1022535 ,\n",
       "        0.01366461,  0.05919865, -0.04841105, -0.10579427, -0.02326384,\n",
       "       -0.00568099,  0.00466576,  0.03079878, -0.0159953 ,  0.01687767,\n",
       "       -0.02068983,  0.05197652,  0.0659098 ,  0.00492441, -0.04337129,\n",
       "        0.01112075, -0.03905351, -0.0141369 ,  0.02856409,  0.07809684,\n",
       "        0.05505262,  0.03897054, -0.04827808,  0.0999007 ,  0.01017398,\n",
       "        0.03616042, -0.019306  , -0.02554031, -0.09940011, -0.01233662,\n",
       "        0.05068025,  0.03322077,  0.02630216,  0.02004424, -0.0467136 ,\n",
       "        0.00642213,  0.04726882,  0.11636643,  0.08925007,  0.01506933,\n",
       "       -0.08074452, -0.00922212, -0.01214509, -0.068476  , -0.07887047,\n",
       "       -0.03523545, -0.00544085, -0.02914756,  0.06558917, -0.01420375,\n",
       "        0.08522833, -0.01341756,  0.01259649, -0.09107063, -0.0130964 ,\n",
       "        0.04440743,  0.11126818,  0.06327965,  0.04317892,  0.11725144,\n",
       "       -0.05215508, -0.06363741, -0.05273328, -0.04113334,  0.02645874,\n",
       "        0.03525361, -0.00417364,  0.04826718,  0.01394653,  0.04811641,\n",
       "       -0.0132588 , -0.07053956,  0.00125958,  0.06180391,  0.02606938,\n",
       "       -0.04646656,  0.06184097, -0.10769054, -0.01288073, -0.04934474,\n",
       "       -0.06416466, -0.00330534, -0.03177897, -0.03413409,  0.00060345],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "def wordvec(data,model):\n",
    "    i = 0.0\n",
    "    out = 0.0\n",
    "    for word in str(data).split():\n",
    "        if word in model.vocab:\n",
    "            out += model[word]\n",
    "            i += 1\n",
    "    if i != 0:\n",
    "        out = out/i\n",
    "    else:\n",
    "        print(data)\n",
    "    return out\n",
    "data = data.drop(['genres','tagline'], axis=1)\n",
    "model2= models.KeyedVectors.load_word2vec_format('./models/GoogleNews.bin',binary=True)\n",
    "data['w_description'] = data['description'].apply(wordvec, args=(model2,))\n",
    "data.iloc[1]['w_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(text,v):\n",
    "    out = 0.0\n",
    "    i2 = 0\n",
    "    j2 = 0\n",
    "    for i,j in zip(text,v):\n",
    "        out = out + i *j\n",
    "        i2 = i2 + i*i\n",
    "        j2 = j2 + j*j\n",
    "    return out, i2,j2\n",
    "maxx = 0\n",
    "idd = 0\n",
    "text = data.iloc[0]['v_description']\n",
    "for i,v in data['v_description'].items():\n",
    "    n,i2,j2 = similar(text,v)\n",
    "    n = n/(i2*j2)**0.5\n",
    "    if i == 0:\n",
    "        continue\n",
    "    elif n < 0.1:\n",
    "        print(\"Similarity =\",n)\n",
    "        print(data.iloc[i]['description'])\n",
    "        print(i)\n",
    "        print(\"_____________________________________________________________________\")\n",
    "    elif n > maxx  and n <0.6768:\n",
    "        maxx = n\n",
    "        idd =i\n",
    "print(\"Similarity =\",maxx)\n",
    "print(data.iloc[idd]['description'])\n",
    "print(idd)\n",
    "print(\"_____________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-english description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4246, 4538, 12769, 13346, 18171, 28408, 29103, 30042, 34244, 34256, 34919, 35314, 36785, 43154, 43533, 44107, 44157, 44859, 45412]\n",
      "(45444, 5)\n"
     ]
    }
   ],
   "source": [
    "rm = []\n",
    "for i,v in data['w_description'].items():\n",
    "    try:\n",
    "        if v == 0.0:\n",
    "            rm.append(i) \n",
    "    except:\n",
    "        pass\n",
    "print(rm)\n",
    "data = data.drop(rm,axis=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sample with no label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sample with no label:  2437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43007, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_genre = []\n",
    "for i,v in data['v_genre'].items():\n",
    "    if v == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:\n",
    "        no_genre.append(i)\n",
    "data = data.drop(no_genre)\n",
    "print(\"Number of sample with no label: \",len(no_genre))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test data\n",
    "    - stratified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05782209  0.03871881  0.00344013  0.08453442  0.00065249  0.01060922\n",
      "  0.05268787 -0.04557037  0.05272275  0.08696493 -0.02750796 -0.11166745\n",
      " -0.02641165 -0.06777664 -0.09281522  0.05298287  0.07704163  0.10824221\n",
      "  0.03521583 -0.05674635  0.05337234 -0.00189427  0.10666484  0.02248201\n",
      "  0.00968474 -0.0568651  -0.07252975  0.08453151  0.09002177 -0.05923752\n",
      " -0.05219232 -0.01393781 -0.04170845  0.01729693 -0.06512742 -0.00980196\n",
      "  0.00528826  0.00168573  0.04685647  0.09290949  0.10045006 -0.04267447\n",
      "  0.12362526  0.03561183  0.00129918 -0.07345654 -0.00184195  0.0200693\n",
      "  0.02873839  0.04418001 -0.06099229  0.11389596  0.0183229  -0.05935669\n",
      "  0.01588694 -0.00556001  0.00986008 -0.18343608  0.04042853 -0.04620434\n",
      "  0.00703076  0.04824611 -0.01732527 -0.0471671  -0.07436553 -0.08427066\n",
      "  0.00455439  0.01441919 -0.03253428  0.13968186  0.09002177  0.01585524\n",
      "  0.06788126 -0.0071244  -0.07991318 -0.04710751  0.03262039  0.03671101\n",
      "  0.00841843  0.03296879  0.00231207 -0.02927253  0.00521478 -0.02015795\n",
      "  0.01407914  0.01751709 -0.10173979  0.06852504  0.00566101  0.04951477\n",
      "  0.01437887  0.02422333 -0.09743936 -0.06567192 -0.01742517 -0.01640829\n",
      "  0.03438168  0.02698117  0.03820001 -0.03092739 -0.04624285 -0.04451352\n",
      "  0.06096613 -0.02759198 -0.0298006  -0.06619263  0.00838797 -0.02178337\n",
      "  0.01969764 -0.08624776 -0.01948039  0.01007153 -0.00876472 -0.00960432\n",
      "  0.09208552 -0.03775914  0.00917126 -0.0218593   0.05016713 -0.01884378\n",
      " -0.04366085 -0.04070754 -0.06656701  0.03381638 -0.03909674 -0.01958429\n",
      "  0.00054205  0.02287075  0.0172348   0.00049073 -0.02625384 -0.04492587\n",
      " -0.06148747  0.02665202 -0.00129918  0.00677187  0.01669203  0.00837853\n",
      " -0.02895555  0.07966977  0.05281676 -0.11279006 -0.05460685  0.05189006\n",
      "  0.0536074   0.08850824 -0.02096376 -0.017318   -0.07119352 -0.08041018\n",
      "  0.08453369 -0.02557809 -0.09414055  0.11020188 -0.01947603 -0.0679047\n",
      " -0.04462387 -0.11795117 -0.08752333 -0.04155986 -0.03166635  0.04334041\n",
      "  0.07853445 -0.09181722 -0.00180163 -0.12309001  0.10754613 -0.03405426\n",
      " -0.02833993  0.02522214 -0.07341222 -0.0279813  -0.02860641 -0.05191476\n",
      " -0.00281098 -0.03130988  0.07749993 -0.05252348 -0.01914687 -0.01799665\n",
      " -0.07824707 -0.06733485  0.05551774 -0.05056572 -0.0271839  -0.02907017\n",
      "  0.00118251  0.01283773  0.08269464  0.01340685  0.08557274 -0.00482523\n",
      "  0.02519771 -0.01879265 -0.03229832  0.07273065 -0.03328741 -0.04183923\n",
      " -0.13810222 -0.06496321  0.05243465  0.01201775 -0.0135211  -0.0704927\n",
      "  0.03764507 -0.06941114  0.01418704  0.00201725 -0.0617683  -0.07694862\n",
      " -0.06490362  0.08935256 -0.05417306 -0.02944874 -0.1022535   0.01366461\n",
      "  0.05919865 -0.04841105 -0.10579427 -0.02326384 -0.00568099  0.00466576\n",
      "  0.03079878 -0.0159953   0.01687767 -0.02068983  0.05197652  0.0659098\n",
      "  0.00492441 -0.04337129  0.01112075 -0.03905351 -0.0141369   0.02856409\n",
      "  0.07809684  0.05505262  0.03897054 -0.04827808  0.0999007   0.01017398\n",
      "  0.03616042 -0.019306   -0.02554031 -0.09940011 -0.01233662  0.05068025\n",
      "  0.03322077  0.02630216  0.02004424 -0.0467136   0.00642213  0.04726882\n",
      "  0.11636643  0.08925007  0.01506933 -0.08074452 -0.00922212 -0.01214509\n",
      " -0.068476   -0.07887047 -0.03523545 -0.00544085 -0.02914756  0.06558917\n",
      " -0.01420375  0.08522833 -0.01341756  0.01259649 -0.09107063 -0.0130964\n",
      "  0.04440743  0.11126818  0.06327965  0.04317892  0.11725144 -0.05215508\n",
      " -0.06363741 -0.05273328 -0.04113334  0.02645874  0.03525361 -0.00417364\n",
      "  0.04826718  0.01394653  0.04811641 -0.0132588  -0.07053956  0.00125958\n",
      "  0.06180391  0.02606938 -0.04646656  0.06184097 -0.10769054 -0.01288073\n",
      " -0.04934474 -0.06416466 -0.00330534 -0.03177897 -0.03413409  0.00060345]\n",
      "(6452, 20)\n",
      "(36555, 300)\n",
      "(36555, 20)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "def split_data(data, proportion):\n",
    "    n = data.shape[0]\n",
    "    tmp = 0\n",
    "    x_test = [] \n",
    "    y_test = []\n",
    "    x_train = [] \n",
    "    y_train = []\n",
    "    train_all = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if n* proportion > tmp:\n",
    "            x_test.append(row['w_description'])\n",
    "            y_test.append(row['v_genre'])\n",
    "        else:\n",
    "            x_train.append(row['w_description'])\n",
    "            y_train.append(row['v_genre'])\n",
    "            train_all.append(row['all'])\n",
    "        tmp+= 1\n",
    "    x_test = array(x_test)\n",
    "    y_test = array(y_test)\n",
    "    x_train = array(x_train)\n",
    "    y_train = array(y_train)\n",
    "    return x_test, y_test , x_train, y_train,train_all\n",
    "\n",
    "def tovec(row,allg):\n",
    "    out = [0]*20\n",
    "    for d in row:\n",
    "        out[allg.index(d)] = 1\n",
    "    return out\n",
    "\n",
    "# data['all'] = data['all'].apply(tovec,args=(allg,))\n",
    "\n",
    "dist = [0] * 20\n",
    "y_int = []\n",
    "for j,y in data['v_genre'].items():\n",
    "    for i,g in zip(range(20),y):\n",
    "        if g == 1:\n",
    "            dist[i] += 1\n",
    "            y_int.append(i)\n",
    "x_test, y_test , x_train, y_train, test_all = split_data(data,0.15)\n",
    "print(x_test[1])\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Doc2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "j = len(documents)\n",
    "for i, d in enumerate(data['description']):\n",
    "    try:\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(d.lower()), tags=[str(i)]))\n",
    "    except:\n",
    "        j+=1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_sizes = [50, 100,200]\n",
    "alpha = 0.025\n",
    "for vec_size in vec_sizes:\n",
    "    model = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "    model.build_vocab(tagged_data)\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        model.alpha -= 0.0002\n",
    "        model.min_alpha = model.alpha\n",
    "    model.save(\"doc2vec.model_\"+str(vec_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feeding Neural Network with basic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162]\n"
     ]
    }
   ],
   "source": [
    "def num_hidden_layer1(num_input, num_output, size):\n",
    "    # alpha is usually set to be a value to be between 2-10\n",
    "    alpha = 10\n",
    "    return [int(size /(num_input + num_output) / alpha)]\n",
    "def num_hidden_layer2(num_input, num_output,size):\n",
    "    # when single hidden layer\n",
    "    return [int(2*((num_output +2)*num_input)**0.5)]\n",
    "def num_hidden_layer3(num_input, num_output,size):\n",
    "    # when two hidden layer\n",
    "    return [int(((num_output+2)*num_input)**0.5 + 2*(num_input/(num_output+2))**0.5), int(num_output*(num_input/(num_output+2))**0.5)]\n",
    "\n",
    "vec_size = 300\n",
    "in_len = vec_size # number of input feature\n",
    "out_len = 20 # number of output label\n",
    "hidden_layer = num_hidden_layer2(in_len,out_len,data.shape[0])\n",
    "activation = 'relu'\n",
    "ep = 100\n",
    "print(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chan Ho Park\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Chan Ho Park\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 36555 samples, validate on 6452 samples\n",
      "Epoch 1/100\n",
      "36555/36555 [==============================] - 3s 85us/step - loss: 2.4065 - acc: 0.2765 - val_loss: 2.1948 - val_acc: 0.2903\n",
      "Epoch 2/100\n",
      "36555/36555 [==============================] - 3s 83us/step - loss: 2.2820 - acc: 0.2917 - val_loss: 2.1499 - val_acc: 0.3007\n",
      "Epoch 3/100\n",
      "36555/36555 [==============================] - 3s 85us/step - loss: 2.2248 - acc: 0.3141 - val_loss: 2.0848 - val_acc: 0.3321\n",
      "Epoch 4/100\n",
      "36555/36555 [==============================] - 3s 77us/step - loss: 2.1593 - acc: 0.3491 - val_loss: 2.0245 - val_acc: 0.3830\n",
      "Epoch 5/100\n",
      "36555/36555 [==============================] - 3s 75us/step - loss: 2.0968 - acc: 0.3755 - val_loss: 1.9721 - val_acc: 0.4011\n",
      "Epoch 6/100\n",
      "36555/36555 [==============================] - 3s 85us/step - loss: 2.0448 - acc: 0.3914 - val_loss: 1.9327 - val_acc: 0.4120\n",
      "Epoch 7/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 2.0042 - acc: 0.4040 - val_loss: 1.8810 - val_acc: 0.4326\n",
      "Epoch 8/100\n",
      "36555/36555 [==============================] - 3s 91us/step - loss: 1.9718 - acc: 0.4111 - val_loss: 1.8700 - val_acc: 0.4344\n",
      "Epoch 9/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.9443 - acc: 0.4199 - val_loss: 1.8468 - val_acc: 0.4445\n",
      "Epoch 10/100\n",
      "36555/36555 [==============================] - 3s 79us/step - loss: 1.9206 - acc: 0.4268 - val_loss: 1.8204 - val_acc: 0.4496\n",
      "Epoch 11/100\n",
      "36555/36555 [==============================] - 3s 79us/step - loss: 1.8989 - acc: 0.4304 - val_loss: 1.8019 - val_acc: 0.4585\n",
      "Epoch 12/100\n",
      "36555/36555 [==============================] - 3s 79us/step - loss: 1.8795 - acc: 0.4367 - val_loss: 1.7844 - val_acc: 0.4613\n",
      "Epoch 13/100\n",
      "36555/36555 [==============================] - 3s 91us/step - loss: 1.8623 - acc: 0.4385 - val_loss: 1.7740 - val_acc: 0.4609\n",
      "Epoch 14/100\n",
      "36555/36555 [==============================] - 3s 85us/step - loss: 1.8467 - acc: 0.4423 - val_loss: 1.7578 - val_acc: 0.4662\n",
      "Epoch 15/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.8326 - acc: 0.4455 - val_loss: 1.7440 - val_acc: 0.4659\n",
      "Epoch 16/100\n",
      "36555/36555 [==============================] - 3s 79us/step - loss: 1.8201 - acc: 0.4480 - val_loss: 1.7416 - val_acc: 0.4636\n",
      "Epoch 17/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.8090 - acc: 0.4492 - val_loss: 1.7281 - val_acc: 0.4719\n",
      "Epoch 18/100\n",
      "36555/36555 [==============================] - 3s 92us/step - loss: 1.7987 - acc: 0.4512 - val_loss: 1.7177 - val_acc: 0.4724\n",
      "Epoch 19/100\n",
      "36555/36555 [==============================] - 4s 101us/step - loss: 1.7896 - acc: 0.4546 - val_loss: 1.7122 - val_acc: 0.4647\n",
      "Epoch 20/100\n",
      "36555/36555 [==============================] - 3s 79us/step - loss: 1.7809 - acc: 0.4564 - val_loss: 1.7019 - val_acc: 0.4730\n",
      "Epoch 21/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.7730 - acc: 0.4572 - val_loss: 1.7043 - val_acc: 0.4678\n",
      "Epoch 22/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.7660 - acc: 0.4576 - val_loss: 1.7024 - val_acc: 0.4657\n",
      "Epoch 23/100\n",
      "36555/36555 [==============================] - 3s 85us/step - loss: 1.7592 - acc: 0.4600 - val_loss: 1.6877 - val_acc: 0.4698\n",
      "Epoch 24/100\n",
      "36555/36555 [==============================] - 3s 86us/step - loss: 1.7531 - acc: 0.4607 - val_loss: 1.6878 - val_acc: 0.4681\n",
      "Epoch 25/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.7470 - acc: 0.4629 - val_loss: 1.6909 - val_acc: 0.4631\n",
      "Epoch 26/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.7416 - acc: 0.4635 - val_loss: 1.6749 - val_acc: 0.4721\n",
      "Epoch 27/100\n",
      "36555/36555 [==============================] - 3s 83us/step - loss: 1.7365 - acc: 0.4642 - val_loss: 1.6734 - val_acc: 0.4690\n",
      "Epoch 28/100\n",
      "36555/36555 [==============================] - 3s 82us/step - loss: 1.7318 - acc: 0.4648 - val_loss: 1.6659 - val_acc: 0.4712\n",
      "Epoch 29/100\n",
      "36555/36555 [==============================] - 3s 93us/step - loss: 1.7272 - acc: 0.4657 - val_loss: 1.6712 - val_acc: 0.4684\n",
      "Epoch 30/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.7230 - acc: 0.4668 - val_loss: 1.6664 - val_acc: 0.4695\n",
      "Epoch 31/100\n",
      "36555/36555 [==============================] - 4s 111us/step - loss: 1.7187 - acc: 0.4675 - val_loss: 1.6612 - val_acc: 0.4719\n",
      "Epoch 32/100\n",
      "36555/36555 [==============================] - 3s 83us/step - loss: 1.7148 - acc: 0.4678 - val_loss: 1.6569 - val_acc: 0.4685\n",
      "Epoch 33/100\n",
      "36555/36555 [==============================] - 3s 84us/step - loss: 1.7107 - acc: 0.4691 - val_loss: 1.6596 - val_acc: 0.4681\n",
      "Epoch 34/100\n",
      "36555/36555 [==============================] - 3s 91us/step - loss: 1.7073 - acc: 0.4675 - val_loss: 1.6490 - val_acc: 0.4693\n",
      "Epoch 35/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.7036 - acc: 0.4693 - val_loss: 1.6458 - val_acc: 0.4721\n",
      "Epoch 36/100\n",
      "36555/36555 [==============================] - 3s 82us/step - loss: 1.7005 - acc: 0.4705 - val_loss: 1.6460 - val_acc: 0.4678\n",
      "Epoch 37/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6969 - acc: 0.4705 - val_loss: 1.6447 - val_acc: 0.4730\n",
      "Epoch 38/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.6941 - acc: 0.4709 - val_loss: 1.6442 - val_acc: 0.4715\n",
      "Epoch 39/100\n",
      "36555/36555 [==============================] - 3s 91us/step - loss: 1.6913 - acc: 0.4707 - val_loss: 1.6455 - val_acc: 0.4704\n",
      "Epoch 40/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.6881 - acc: 0.4719 - val_loss: 1.6375 - val_acc: 0.4732\n",
      "Epoch 41/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6853 - acc: 0.4721 - val_loss: 1.6402 - val_acc: 0.4687\n",
      "Epoch 42/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6827 - acc: 0.4730 - val_loss: 1.6372 - val_acc: 0.4702\n",
      "Epoch 43/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6799 - acc: 0.4735 - val_loss: 1.6384 - val_acc: 0.4712\n",
      "Epoch 44/100\n",
      "36555/36555 [==============================] - 3s 92us/step - loss: 1.6778 - acc: 0.4733 - val_loss: 1.6339 - val_acc: 0.4687\n",
      "Epoch 45/100\n",
      "36555/36555 [==============================] - 4s 97us/step - loss: 1.6752 - acc: 0.4741 - val_loss: 1.6362 - val_acc: 0.4692\n",
      "Epoch 46/100\n",
      "36555/36555 [==============================] - 4s 96us/step - loss: 1.6728 - acc: 0.4741 - val_loss: 1.6284 - val_acc: 0.4712\n",
      "Epoch 47/100\n",
      "36555/36555 [==============================] - 3s 83us/step - loss: 1.6704 - acc: 0.4759 - val_loss: 1.6327 - val_acc: 0.4730\n",
      "Epoch 48/100\n",
      "36555/36555 [==============================] - 3s 83us/step - loss: 1.6682 - acc: 0.4751 - val_loss: 1.6304 - val_acc: 0.4726\n",
      "Epoch 49/100\n",
      "36555/36555 [==============================] - 3s 95us/step - loss: 1.6661 - acc: 0.4764 - val_loss: 1.6245 - val_acc: 0.4747\n",
      "Epoch 50/100\n",
      "36555/36555 [==============================] - 3s 83us/step - loss: 1.6641 - acc: 0.4766 - val_loss: 1.6241 - val_acc: 0.4744\n",
      "Epoch 51/100\n",
      "36555/36555 [==============================] - 3s 84us/step - loss: 1.6622 - acc: 0.4780 - val_loss: 1.6355 - val_acc: 0.4732\n",
      "Epoch 52/100\n",
      "36555/36555 [==============================] - 3s 84us/step - loss: 1.6601 - acc: 0.4777 - val_loss: 1.6263 - val_acc: 0.4721\n",
      "Epoch 53/100\n",
      "36555/36555 [==============================] - 3s 86us/step - loss: 1.6582 - acc: 0.4789 - val_loss: 1.6212 - val_acc: 0.4754\n",
      "Epoch 54/100\n",
      "36555/36555 [==============================] - 4s 97us/step - loss: 1.6565 - acc: 0.4794 - val_loss: 1.6239 - val_acc: 0.4693\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36555/36555 [==============================] - 3s 78us/step - loss: 1.6546 - acc: 0.4807 - val_loss: 1.6221 - val_acc: 0.4709\n",
      "Epoch 56/100\n",
      "36555/36555 [==============================] - 3s 77us/step - loss: 1.6526 - acc: 0.4800 - val_loss: 1.6182 - val_acc: 0.4740\n",
      "Epoch 57/100\n",
      "36555/36555 [==============================] - 3s 77us/step - loss: 1.6513 - acc: 0.4790 - val_loss: 1.6167 - val_acc: 0.4682\n",
      "Epoch 58/100\n",
      "36555/36555 [==============================] - 3s 78us/step - loss: 1.6493 - acc: 0.4818 - val_loss: 1.6166 - val_acc: 0.4735\n",
      "Epoch 59/100\n",
      "36555/36555 [==============================] - 3s 82us/step - loss: 1.6478 - acc: 0.4815 - val_loss: 1.6163 - val_acc: 0.4696\n",
      "Epoch 60/100\n",
      "36555/36555 [==============================] - 3s 82us/step - loss: 1.6461 - acc: 0.4814 - val_loss: 1.6099 - val_acc: 0.4723\n",
      "Epoch 61/100\n",
      "36555/36555 [==============================] - 3s 78us/step - loss: 1.6449 - acc: 0.4815 - val_loss: 1.6210 - val_acc: 0.4712\n",
      "Epoch 62/100\n",
      "36555/36555 [==============================] - 3s 78us/step - loss: 1.6430 - acc: 0.4810 - val_loss: 1.6144 - val_acc: 0.4721\n",
      "Epoch 63/100\n",
      "36555/36555 [==============================] - 3s 78us/step - loss: 1.6414 - acc: 0.4820 - val_loss: 1.6160 - val_acc: 0.4723\n",
      "Epoch 64/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.6400 - acc: 0.4825 - val_loss: 1.6105 - val_acc: 0.4696\n",
      "Epoch 65/100\n",
      "36555/36555 [==============================] - 3s 92us/step - loss: 1.6386 - acc: 0.4824 - val_loss: 1.6114 - val_acc: 0.4713\n",
      "Epoch 66/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6371 - acc: 0.4829 - val_loss: 1.6132 - val_acc: 0.4730\n",
      "Epoch 67/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6354 - acc: 0.4837 - val_loss: 1.6060 - val_acc: 0.4740\n",
      "Epoch 68/100\n",
      "36555/36555 [==============================] - 3s 81us/step - loss: 1.6342 - acc: 0.4846 - val_loss: 1.6288 - val_acc: 0.4630\n",
      "Epoch 69/100\n",
      "36555/36555 [==============================] - 3s 80us/step - loss: 1.6329 - acc: 0.4850 - val_loss: 1.6046 - val_acc: 0.4719\n",
      "Epoch 70/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6316 - acc: 0.4833 - val_loss: 1.6222 - val_acc: 0.4698\n",
      "Epoch 71/100\n",
      "36555/36555 [==============================] - 4s 108us/step - loss: 1.6303 - acc: 0.4847 - val_loss: 1.6051 - val_acc: 0.4730\n",
      "Epoch 72/100\n",
      "36555/36555 [==============================] - 3s 87us/step - loss: 1.6289 - acc: 0.4848 - val_loss: 1.6058 - val_acc: 0.4737\n",
      "Epoch 73/100\n",
      "36555/36555 [==============================] - 3s 91us/step - loss: 1.6277 - acc: 0.4853 - val_loss: 1.6026 - val_acc: 0.4721\n",
      "Epoch 74/100\n",
      "36555/36555 [==============================] - 3s 88us/step - loss: 1.6264 - acc: 0.4855 - val_loss: 1.6168 - val_acc: 0.4676\n",
      "Epoch 75/100\n",
      "36555/36555 [==============================] - 4s 101us/step - loss: 1.6251 - acc: 0.4870 - val_loss: 1.6115 - val_acc: 0.4698\n",
      "Epoch 76/100\n",
      "36555/36555 [==============================] - 3s 90us/step - loss: 1.6239 - acc: 0.4852 - val_loss: 1.6047 - val_acc: 0.4744\n",
      "Epoch 77/100\n",
      "36555/36555 [==============================] - 3s 89us/step - loss: 1.6229 - acc: 0.4855 - val_loss: 1.6118 - val_acc: 0.4696\n",
      "Epoch 78/100\n",
      "36555/36555 [==============================] - 3s 89us/step - loss: 1.6218 - acc: 0.4858 - val_loss: 1.6056 - val_acc: 0.4707\n",
      "Epoch 79/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6203 - acc: 0.4886 - val_loss: 1.6096 - val_acc: 0.4715\n",
      "Epoch 80/100\n",
      "36555/36555 [==============================] - 4s 101us/step - loss: 1.6193 - acc: 0.4860 - val_loss: 1.6077 - val_acc: 0.4735\n",
      "Epoch 81/100\n",
      "36555/36555 [==============================] - 3s 92us/step - loss: 1.6181 - acc: 0.4867 - val_loss: 1.6059 - val_acc: 0.4704\n",
      "Epoch 82/100\n",
      "36555/36555 [==============================] - 3s 92us/step - loss: 1.6170 - acc: 0.4888 - val_loss: 1.6021 - val_acc: 0.4729\n",
      "Epoch 83/100\n",
      "36555/36555 [==============================] - 3s 95us/step - loss: 1.6159 - acc: 0.4874 - val_loss: 1.6101 - val_acc: 0.4723\n",
      "Epoch 84/100\n",
      "36555/36555 [==============================] - 4s 109us/step - loss: 1.6145 - acc: 0.4876 - val_loss: 1.6114 - val_acc: 0.4678\n",
      "Epoch 85/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6137 - acc: 0.4883 - val_loss: 1.6019 - val_acc: 0.4721\n",
      "Epoch 86/100\n",
      "36555/36555 [==============================] - 3s 93us/step - loss: 1.6123 - acc: 0.4895 - val_loss: 1.6033 - val_acc: 0.4712\n",
      "Epoch 87/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6111 - acc: 0.4893 - val_loss: 1.6024 - val_acc: 0.4733\n",
      "Epoch 88/100\n",
      "36555/36555 [==============================] - 3s 95us/step - loss: 1.6102 - acc: 0.4897 - val_loss: 1.6080 - val_acc: 0.4721\n",
      "Epoch 89/100\n",
      "36555/36555 [==============================] - 4s 103us/step - loss: 1.6094 - acc: 0.4894 - val_loss: 1.5933 - val_acc: 0.4769\n",
      "Epoch 90/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6083 - acc: 0.4898 - val_loss: 1.5986 - val_acc: 0.4741\n",
      "Epoch 91/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6073 - acc: 0.4899 - val_loss: 1.5918 - val_acc: 0.4726\n",
      "Epoch 92/100\n",
      "36555/36555 [==============================] - 3s 95us/step - loss: 1.6063 - acc: 0.4887 - val_loss: 1.5941 - val_acc: 0.4749\n",
      "Epoch 93/100\n",
      "36555/36555 [==============================] - 4s 105us/step - loss: 1.6051 - acc: 0.4911 - val_loss: 1.6085 - val_acc: 0.4690\n",
      "Epoch 94/100\n",
      "36555/36555 [==============================] - 3s 96us/step - loss: 1.6040 - acc: 0.4912 - val_loss: 1.6034 - val_acc: 0.4723\n",
      "Epoch 95/100\n",
      "36555/36555 [==============================] - 3s 94us/step - loss: 1.6031 - acc: 0.4898 - val_loss: 1.6049 - val_acc: 0.4724\n",
      "Epoch 96/100\n",
      "36555/36555 [==============================] - 3s 95us/step - loss: 1.6025 - acc: 0.4892 - val_loss: 1.5980 - val_acc: 0.4724\n",
      "Epoch 97/100\n",
      "36555/36555 [==============================] - 3s 96us/step - loss: 1.6012 - acc: 0.4916 - val_loss: 1.6044 - val_acc: 0.4715\n",
      "Epoch 98/100\n",
      "36555/36555 [==============================] - 4s 108us/step - loss: 1.6001 - acc: 0.4909 - val_loss: 1.5962 - val_acc: 0.4747\n",
      "Epoch 99/100\n",
      "36555/36555 [==============================] - 3s 95us/step - loss: 1.5990 - acc: 0.4912 - val_loss: 1.5956 - val_acc: 0.4741\n",
      "Epoch 100/100\n",
      "36555/36555 [==============================] - 4s 96us/step - loss: 1.5983 - acc: 0.4910 - val_loss: 1.5929 - val_acc: 0.4729\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "if len(hidden_layer) == 1:\n",
    "    nn.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "elif len(hidden_layer) == 2:\n",
    "    nn.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "    nn.add(Dense(hidden_layer[1],activation=activation))\n",
    "elif len(hidden_layer) == 3:\n",
    "    nn.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "    nn.add(Dense(hidden_layer[1],activation=activation))\n",
    "    nn.add(Dense(hidden_layer[2],activation=activation))\n",
    "nn.add(Dense(out_len, activation='softmax'))\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "history = nn.fit(x_train, y_train, epochs=ep, batch_size=20, verbose=1, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np \n",
    "from numpy import array\n",
    "import keras as K\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(20)\n",
    "    plt.xticks(tick_marks, allg[:20], rotation=45)\n",
    "    plt.yticks(tick_marks, allg[:20])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "test_pred = nn.predict(x_test)\n",
    "conv_y = []\n",
    "conv_pred = []\n",
    "for y in y_test:\n",
    "    conv_y.append(np.argmax(y))\n",
    "for y in test_pred:\n",
    "    conv_pred.append(np.argmax(y))\n",
    "title =\"Confusion matrix\"\n",
    "cm = confusion_matrix(conv_y,conv_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(12,10))\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg = Sequential()\n",
    "if len(hidden_layer) == 1:\n",
    "    nn.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation,kernel_regularizer=l2(0.1)))\n",
    "elif len(hidden_layer) == 2:\n",
    "    nn_reg.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation, kernel_regularizer=l2(0.01)))\n",
    "    nn_reg.add(Dense(hidden_layer[1],activation=activation))\n",
    "nn_reg.add(Dense(out_len, activation='softmax'))\n",
    "nn_reg.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "history_reg = nn_reg.fit(x_train, y_train, epochs=ep, batch_size=20, verbose=1, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weight / Label penalty \n",
    "\n",
    "If you are talking about the regular case, where your network produces only one output, then your assumption is correct.\n",
    "In order to force your algorithm to treat every instance of class 1 as 50 instances of class 0 you have to:\n",
    "class_weight = {0: 1.,\n",
    "                1: 50.,\n",
    "                2: 2.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# method1 \n",
    "# dist = [0] * 20\n",
    "# for y in y_train:\n",
    "#     for i,g in zip(range(20),y):\n",
    "#         if g == 1:\n",
    "#             dist[i] += 1\n",
    "# s = 0\n",
    "# for d in dist:\n",
    "#     s += d\n",
    "# weight_dic = {}\n",
    "\n",
    "#method2 \n",
    "dist = [0] * 20\n",
    "y_train_int = []\n",
    "# convert vector label to int label for SMOTE\n",
    "for y in y_train:\n",
    "    for i,g in zip(range(20),y):\n",
    "        if g == 1:\n",
    "            dist[i] += 1\n",
    "            y_train_int.append(i)\n",
    "weights = compute_class_weight('balanced', range(20), y_train_int)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dic = {}\n",
    "for i,w in enumerate(weights):\n",
    "    weight_dic[i] = w\n",
    "print(\"Class weight is : \\n\",weight_dic)\n",
    "\n",
    "nn_weight = Sequential()\n",
    "if len(hidden_layer) == 1:\n",
    "    nn_weight.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "elif len(hidden_layer) == 2:\n",
    "    nn_weight.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "    nn_weight.add(Dense(hidden_layer[1],activation=activation))\n",
    "nn_weight.add(Dense(out_len, activation='softmax'))\n",
    "nn_weight.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "history_class = nn_weight.fit(x_train, y_train, epochs=100, batch_size=20, verbose=1, validation_data=(x_test, y_test), class_weight=weight_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np \n",
    "from numpy import array\n",
    "import keras as K\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(20)\n",
    "    plt.xticks(tick_marks, allg[:20], rotation=45)\n",
    "    plt.yticks(tick_marks, allg[:20])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "test_pred = nn_weight.predict(x_test)\n",
    "conv_y = []\n",
    "conv_pred = []\n",
    "for y in y_test:\n",
    "    conv_y.append(np.argmax(y))\n",
    "for y in test_pred:\n",
    "    conv_pred.append(np.argmax(y))\n",
    "title =\"Confusion matrix\"\n",
    "cm = confusion_matrix(conv_y,conv_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(12,10))\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling/ SMOTE data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Genres in Dataset / Resampling (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "dist = [0] * 20\n",
    "y_train_int = []\n",
    "# convert vector label to int label for SMOTE\n",
    "for y in y_train:\n",
    "    for i,g in zip(range(20),y):\n",
    "        if g == 1:\n",
    "            dist[i] += 1\n",
    "            y_train_int.append(i)\n",
    "\n",
    "# check distribution of labels\n",
    "s= 0\n",
    "for d in dist:\n",
    "    s += d\n",
    "for d,g in zip(dist,allg):\n",
    "    print(\"%.2f \"%(d/s) + g)\n",
    "\n",
    "x_resampled, y_resampled = SMOTE().fit_resample(x_train, y_train_int)\n",
    "\n",
    "# check distribution after oversampling\n",
    "print(\"After SMOTE \\n\")\n",
    "dist = [0] * 20\n",
    "for y in y_resampled:\n",
    "    dist[y] += 1\n",
    "s = 0\n",
    "for d in dist:\n",
    "    s += d\n",
    "for d,g in zip(dist,allg):\n",
    "    print(\"%.2f \"%(d/s) + g)\n",
    "\n",
    "# X_resampled, y_resampled = ADASYN().fit_resample(x_train, y)\n",
    "#back to one hot encoding\n",
    "y_resampled_conv = []\n",
    "for y in y_resampled:\n",
    "    add = [0] * 20\n",
    "    add[y] = 1\n",
    "    y_resampled_conv.append(add)\n",
    "y_resampled_conv = array(y_resampled_conv)\n",
    "print(y_resampled_conv[1])\n",
    "nn = Sequential()\n",
    "if len(hidden_layer) == 1:\n",
    "    nn.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "elif len(hidden_layer) == 2:\n",
    "    nn.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "    nn.add(Dense(hidden_layer[1],activation=activation))\n",
    "nn.add(Dense(out_len, activation='softmax'))\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_re = nn.fit(x_resampled, y_resampled_conv, epochs=ep, batch_size=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np \n",
    "from numpy import array\n",
    "import keras as K\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(20)\n",
    "    plt.xticks(tick_marks, allg[:20], rotation=45)\n",
    "    plt.yticks(tick_marks, allg[:20])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "test_pred = nn.predict(x_test)\n",
    "conv_y = []\n",
    "conv_pred = []\n",
    "for y in y_test:\n",
    "    conv_y.append(np.argmax(y))\n",
    "for y in test_pred:\n",
    "    conv_pred.append(np.argmax(y))\n",
    "title =\"Confusion matrix\"\n",
    "cm = confusion_matrix(conv_y,conv_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(12,10))\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"./data/movies_metadata.csv\"\n",
    "data2 = pd.read_csv(file_dir)\n",
    "data2 = data2['genres']\n",
    "\n",
    "rm = []\n",
    "all_label = []\n",
    "for i,row in data2.items():\n",
    "    dic = ast.literal_eval(row)  \n",
    "    dic = ast.literal_eval(row)  \n",
    "    out = []\n",
    "    for g in dic:\n",
    "        out.append(g['name'])\n",
    "    if len(out) == 0:\n",
    "        rm.append(i)\n",
    "    all_label.append(out)\n",
    "data2 = data2.drop([19730, 29503, 35587, 4246, 4538, 12769, 13346, 28408, 29103, 30042, 34244, 34256, 36785, 43154, 44107, 44157, 45412])\n",
    "data2 = data2.drop(rm)\n",
    "data2.shape\n",
    "data2.head(5)\n",
    "print(all_label[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five prediction : ['Action', 'Horror', 'Comedy', 'Drama', 'Comedy']\n",
      "True genre of first five\n",
      "[Animation, Comedy, Family] \n",
      "[Adventure, Fantasy, Family]\n",
      "[Romance, Comedy]\n",
      "[Comedy, Drama, Romance]\n",
      "[Comedy]\n",
      "\n",
      "0.15316646149637533\n"
     ]
    }
   ],
   "source": [
    "def in_max(l):\n",
    "    m = 0\n",
    "    out = 0\n",
    "    for i,j in zip(range(20),l):\n",
    "        if m < j:\n",
    "            m = j\n",
    "            out = i\n",
    "    return out\n",
    "def score(true,final):\n",
    "    correct = 0\n",
    "    for i,j in zip(true,final):\n",
    "        try:\n",
    "            i.index(j)\n",
    "            correct +=1\n",
    "        except:\n",
    "            continue\n",
    "    print(correct/len(true))\n",
    "\n",
    "pred_weight = nn.predict(x_train)\n",
    "pred_nn = nn.predict(x_train)\n",
    "final = []\n",
    "for r1,r2 in zip(pred_weight,pred_nn):\n",
    "    add = []\n",
    "    for v1,v2 in zip(r1,r2):\n",
    "        add.append((v1+v2)/2)\n",
    "    final.append(in_max(add))\n",
    "print(\"First five prediction :\",[allg[i] for i in final[0:5]])\n",
    "final = array(final)\n",
    "true = []\n",
    "#print(test_all[1:5])\n",
    "for i in test_all:\n",
    "    add =[]\n",
    "    for p,q in zip(range(20),i):\n",
    "        if q != 0:\n",
    "            add.append(p)\n",
    "    true.append(add)\n",
    "print(\"True genre of first five\\n\"+\n",
    "'''[Animation, Comedy, Family] \n",
    "[Adventure, Fantasy, Family]\n",
    "[Romance, Comedy]\n",
    "[Comedy, Drama, Romance]\n",
    "[Comedy]\n",
    "''')\n",
    "score(true,final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.keras import balanced_batch_generator\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "\n",
    "training_generator, steps_per_epoch = balanced_batch_generator(\n",
    "                                        x_train, y_train, sampler=NearMiss(), batch_size=10, random_state=42)\n",
    "\n",
    "nn_under = Sequential()\n",
    "if len(hidden_layer) == 1:\n",
    "    nn_under.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "elif len(hidden_layer) == 2:\n",
    "    nn_under.add(Dense(hidden_layer[0],input_dim=in_len,activation=activation))\n",
    "    nn_under.add(Dense(hidden_layer[1],activation=activation))\n",
    "nn_under.add(Dense(out_len, activation='softmax'))\n",
    "nn_under.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "under_hist = nn_under.fit_generator(generator=training_generator,\n",
    "                                        steps_per_epoch=steps_per_epoch,\n",
    "                                        epochs=150, verbose=1)\n",
    "print(nn_under.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np \n",
    "from numpy import array\n",
    "import keras as K\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(20)\n",
    "    plt.xticks(tick_marks, allg[:20], rotation=45)\n",
    "    plt.yticks(tick_marks, allg[:20])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "test_pred = nn_under.predict(x_test)\n",
    "conv_y = []\n",
    "conv_pred = []\n",
    "for y in y_test:\n",
    "    conv_y.append(np.argmax(y))\n",
    "for y in test_pred:\n",
    "    conv_pred.append(np.argmax(y))\n",
    "title =\"Confusion matrix\"\n",
    "cm = confusion_matrix(conv_y,conv_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(12,10))\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two models\n",
    "\n",
    "def final_pred(nn_model, cnn_model,x_test,y_test):\n",
    "    nn_pred = nn_model.predict(x_test)\n",
    "    cnn_pred = cnn_model.predict(x_test)\n",
    "    pred = []\n",
    "    for i,j in zip(nn_pred,cnn_pred):\n",
    "        sample_pred = []\n",
    "        for p1,p2 in zip(i,j):\n",
    "            sample_pred.append((i+j)/2)\n",
    "        sample_pred[np.argmax()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
